


import numpy as np
import torch
from torch import nn
import matplotlib.pyplot as plt
from torch import nn, optim, autograd
from torch.nn import functional as F


from scipy import io
y = dict()
y['low'] = io.loadmat('20x20_100_CH.mat')['CH']
y['hi'] = io.loadmat('200x200_100_CH.mat')['CH']








class Unit(nn.Module):

    def __init__(self, in_N, out_N):
        super(Unit, self).__init__()
        self.in_N = in_N
        self.out_N = out_N
        self.L = nn.Linear(in_N, out_N)

    def forward(self, x):
        x1 = self.L(x)
        x2 = torch.tanh(x1)
        return x2


class NN1(nn.Module):

    def __init__(self, in_N, width, depth, out_N):
        super(NN1, self).__init__()
        self.width = width
        self.in_N = in_N
        self.out_N = out_N
        self.stack = nn.ModuleList()

        self.stack.append(Unit(in_N, width))

        for i in range(depth):
            self.stack.append(Unit(width, width))

        self.stack.append(nn.Linear(width, out_N))

    def forward(self, x):
        # first layer
        for i in range(len(self.stack)):
            x = self.stack[i](x)
        return x


class NN2(nn.Module):
    def __init__(self, in_N, width, depth, out_N):
        super(NN2, self).__init__()
        self.in_N = in_N
        self.width = width
        self.depth = depth
        self.out_N = out_N

        self.stack = nn.ModuleList()

        self.stack.append(nn.Linear(in_N, width))

        for i in range(depth):
            self.stack.append(nn.Linear(width, width))

        self.stack.append(nn.Linear(width, out_N))

    def forward(self, x):
        # first layer
        for i in range(len(self.stack)):
            x = self.stack[i](x)
        return x

def weights_init(m):
    if isinstance(m, (nn.Conv2d, nn.Linear)):
        nn.init.xavier_normal_(m.weight)
        nn.init.constant_(m.bias, 0.0)



xx = np.linspace(0.1, 1, 100)
# (1,2) element of CH
yy = y['hi'][:,0,1]

train_index = [int(np.floor(i)) for i in np.linspace(0,99,5)]

x_train = xx[train_index].reshape((-1, 1))
y_train = yy[train_index].reshape((-1, 1))



# dnn
in_N = 1
width = 10
depth = 2
out_N = 1


model_h = NN1(in_N, width, depth, out_N)
model_h.apply(weights_init)
optimizer = optim.Adam(model_h.parameters(), lr=0.001)
nIter = 5000
it = 0
loss_value = 1
while loss_value > 1e-4:
    pred_h = model_h(torch.from_numpy(x_train).float())
    loss = torch.mean(torch.square(pred_h - torch.from_numpy(y_train).float()))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_value = loss.item()
    if it % 50 == 0:
        print('It:', it, 'Loss', loss.item())
    it = it + 1
nn_pred_h = model_h(torch.from_numpy(xx.reshape((-1, 1))).float())


x = range(100)
plt.figure()

plt.plot(xx, y['hi'][x,0,1],color='red', linewidth=2.0,label='hi_ref')
plt.plot(xx, nn_pred_h.detach().numpy(),color='blue', linewidth=2.0,label='hi_nn')
plt.scatter(x_train.reshape(1,-1), y_train.reshape(1,-1),color='black', marker='x',label='sample_points')

plt.grid(True)
plt.show()


train_index = [int(np.floor(i)) for i in np.linspace(0,99,20)]

x_train = xx[train_index].reshape((-1, 1))
y_train = yy[train_index].reshape((-1, 1))


# dnn
in_N = 1
width = 10
depth = 2
out_N = 1


model_h = NN1(in_N, width, depth, out_N)
model_h.apply(weights_init)
optimizer = optim.Adam(model_h.parameters(), lr=0.001)
nIter = 5000
it = 0
loss_value = 1
while loss_value > 1e-4:
    pred_h = model_h(torch.from_numpy(x_train).float())
    loss = torch.mean(torch.square(pred_h - torch.from_numpy(y_train).float()))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_value = loss.item()
    if it % 50 == 0:
        print('It:', it, 'Loss', loss.item())
    it = it + 1
nn_pred_h = model_h(torch.from_numpy(xx.reshape((-1, 1))).float())


plt.figure()

plt.plot(xx, y['hi'][x,0,1],color='red', linewidth=2.0,label='hi_ref')
plt.plot(xx, nn_pred_h.detach().numpy(),color='blue', linewidth=2.0,label='hi_nn')
plt.scatter(x_train.reshape(1,-1), y_train.reshape(1,-1),color='black', marker='x',label='sample_points')

plt.grid(True)
plt.show()





x = np.linspace(0.1, 1, 100)
# (1,1) element of CH
y_hi = y['hi'][:,0,1]
y_lo = y['low'][:,0,1]
train_index_lo = [int(np.floor(i)) for i in np.linspace(0,99,20)]
train_index_hi = [int(np.floor(i)) for i in np.linspace(0,99,5)]


x_train_lo = x[train_index_lo].reshape(-1, 1)
y_train_lo = y_lo[train_index_lo].reshape(-1, 1)



# dnn
# model_L: nn_l
model_L = NN1(1, 20, 4, 1)
model_L.apply(weights_init)
optimizer = optim.Adam(model_L.parameters(), lr=1e-3)
loss_value = 1
x_lo_r = torch.from_numpy(x_train_lo).float()
x_lo_r.requires_grad_()
it = 0
while loss_value > 5e-5:
    pred_h = model_L(x_lo_r)
    grads = autograd.grad(outputs=pred_h, inputs=x_lo_r,
                            grad_outputs=torch.ones_like(pred_h),
                            create_graph=True, retain_graph=True, only_inputs=True)[0]
    loss = torch.mean(torch.square(pred_h - torch.from_numpy(y_train_lo).float()))
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_value = loss.item()
    if it % 50 == 0:
        print('It:', it, 'Loss', loss.item())
    it = it + 1



nn_pred_l = model_L(torch.from_numpy(x.reshape((-1, 1))).float())
plt.figure()
plt.plot(x, y_lo,color='red', linewidth=2.0,label='hi_ref')
plt.plot(x, nn_pred_l.detach().numpy(),color='blue', linewidth=2.0,label='hi_nn')
plt.scatter(x_train_lo.reshape(1,-1), y_train_lo.reshape(1,-1),color='black', marker='x',label='sample_points')

plt.grid(True)
plt.show()


xx = np.linspace(0.1, 1, 100)
# (1,1) element of CH
yy_hi = y['hi'][:,0,1]
yy_lo = y['low'][:,0,1]

train_index_lo = [int(np.floor(i)) for i in np.linspace(0,99,30)]
# train_index_hi = [int(np.floor(i)) for i in np.linspace(0,99,6)]
train_index_hi = [0,29,59,89,99]
x_train_lo = xx[train_index_lo].reshape((-1, 1))
y_train_lo = yy_lo[train_index_lo].reshape((-1, 1))
x_train_hi = xx[train_index_hi].reshape((-1, 1))
y_train_hi = yy_hi[train_index_hi].reshape((-1, 1))
x_test = xx
y_test_lo = yy_lo
y_test_hi = yy_hi



# dnn with linear sub nn
model_noLinear = NN1(2, 30, 3, 1)
model_noLinear.apply(weights_init)
model_Linear = NN2(2, 10, 1, 1)
model_Linear.apply(weights_init)
alpha = torch.tensor([0.1])
# model_Alpha = nn.Linear(2,1,bias=False)
optimizer2 = optim.Adam([{'params': model_noLinear.parameters()},
                         {'params': model_Linear.parameters()},
                         # {'params': model_L.parameters(), 'weight_decay': 0.01},
                          {'params': alpha}], lr=1e-3)
nIter2 = 10000
x_lo_r = torch.from_numpy(x_train_lo).float()
x_lo_r.requires_grad_()
loss2_value = 1
it = 0
# x_lo_r.requires_grad_()
while loss2_value > 1e-5 and it < nIter2:
    # pred_h = model_L(x_lo_r)
    # grads = autograd.grad(outputs=pred_h, inputs=x_lo_r,
    #                         grad_outputs=torch.ones_like(pred_h),
    #                         create_graph=True, retain_graph=True, only_inputs=True)[0]
    # loss3 = torch.mean(torch.square(pred_h - torch.from_numpy(y_train_lo).float()))

    pred_2h = model_L(torch.from_numpy(x_train_hi).float())
    pred_2 = alpha * model_noLinear(torch.cat((torch.from_numpy(x_train_hi).float(), pred_2h), 1)) +\
              (1 - alpha) * model_Linear(torch.cat((torch.from_numpy(x_train_hi).float(), pred_2h), 1))
    # pred_nl = model_noLinear(torch.cat((torch.from_numpy(x_train_hi).float(), pred_2h), 1))
    # pred_l = model_Linear(torch.cat((torch.from_numpy(x_train_hi).float(), pred_2h), 1))
    
    # pred_2 = model_Alpha(torch.cat((pred_nl,pred_l),1))
               
    # loss2 = torch.mean(torch.square(pred_2 - torch.from_numpy(y_train_hi))) + loss3
    loss2 = torch.mean(torch.square(pred_2 - torch.from_numpy(y_train_hi))) 
    loss2_value = loss2.item()
    optimizer2.zero_grad()
    loss2.backward()
    optimizer2.step()
    if it % 100 == 0:
        print('It:', it, 'Loss:', loss2.item())
    it = it + 1




y_lo_nn = model_L(torch.from_numpy(x_test.reshape((-1,1))).float())
y_hi_mfnn = alpha * model_noLinear(torch.cat((torch.from_numpy(x_test.reshape((-1,1))).float(), y_lo_nn), 1)) +\
              (1 - alpha) * model_Linear(torch.cat((torch.from_numpy(x_test.reshape((-1,1))).float(), y_lo_nn), 1))
# y_hi_mfnn = model_Alpha(torch.cat((model_noLinear(torch.cat((torch.from_numpy(x_test.reshape((-1,1))).float(), y_lo_nn), 1)),
#   model_Linear(torch.cat((torch.from_numpy(x_test.reshape((-1,1))).float(), y_lo_nn), 1))),1))
    


fig4, ax4 = plt.subplots(figsize=(10,10))
line1 = ax4.plot(x_test, y_test_lo, label='ref_lo',ls='--', color='blue')
line2 = ax4.plot(x_test, y_test_hi, label='ref_hi',ls='--', color='red')
line3 = ax4.plot(x_test, y_hi_mfnn.detach().numpy(), label='MFNN',ls='-', color='darkviolet')
line4 = ax4.plot(x_test, y_lo_nn.detach().numpy(), label='nn_lo',ls='-', color='green')

ax4.scatter(x_train_lo, y_train_lo, marker='x', color='red', linewidth=2)
ax4.scatter(x_train_hi, y_train_hi, marker='o', color='black', linewidth=2)
ax4.legend()
plt.show()


pred_2


y_train_hi



