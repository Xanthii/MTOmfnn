import numpy as np
import torch
from torch import nn
import matplotlib.pyplot as plt
from torch import nn, optim, autograd
from torch.nn import functional as F





class Unit(nn.Module):

    def __init__(self, in_N, out_N):
        super(Unit, self).__init__()
        self.in_N = in_N
        self.out_N = out_N
        self.L = nn.Linear(in_N, out_N)

    def forward(self, x):
        x1 = self.L(x)
        x2 = torch.tanh(x1)
        return x2


class NN1(nn.Module):

    def __init__(self, in_N, width, depth, out_N):
        super(NN1, self).__init__()
        self.width = width
        self.in_N = in_N
        self.out_N = out_N
        self.stack = nn.ModuleList()

        self.stack.append(Unit(in_N, width))

        for i in range(depth):
            self.stack.append(Unit(width, width))

        self.stack.append(nn.Linear(width, out_N))

    def forward(self, x):
        # first layer
        for i in range(len(self.stack)):
            x = self.stack[i](x)
        return x


class NN2(nn.Module):
    def __init__(self, in_N, width, depth, out_N):
        super(NN2, self).__init__()
        self.in_N = in_N
        self.width = width
        self.depth = depth
        self.out_N = out_N

        self.stack = nn.ModuleList()

        self.stack.append(nn.Linear(in_N, width))

        for i in range(depth):
            self.stack.append(nn.Linear(width, width))

        self.stack.append(nn.Linear(width, out_N))

    def forward(self, x):
        # first layer
        for i in range(len(self.stack)):
            x = self.stack[i](x)
        return x

def weights_init(m):
    if isinstance(m, (nn.Conv2d, nn.Linear)):
        nn.init.xavier_normal_(m.weight)
        nn.init.constant_(m.bias, 0.0)






from scipy import io
xx = np.linspace(0,1,100)
x = range(100)
y = dict()
rpath = '../micro_data/2d_crossed/training_data/'
y['low'] = io.loadmat(rpath+'crossed_20x20_100_CH.mat')['CH']
y['hi'] = io.loadmat(rpath+'crossed_200x200_100_CH.mat')['CH']


# plot
fig, axes = plt.subplots(2,2,figsize=(15, 15))

axes[0,0].plot(xx, y['low'][x,0,0],color='blue', linewidth=2.0,label='low')
axes[0,0].plot(xx, y['hi'][x,0,0],color='red', linewidth=2.0,label='hi')
axes[0,0].legend()
axes[0,1].plot(xx, y['low'][x,0,1],color='blue', linewidth=2.0,label='low')
axes[0,1].plot(xx, y['hi'][x,0,1],color='red', linewidth=2.0,label='hi')
axes[0,1].legend()
axes[1,0].plot(xx, y['low'][x,2,2],color='blue', linewidth=2.0,label='low')
axes[1,0].plot(xx, y['hi'][x,2,2],color='red', linewidth=2.0,label='hi')
axes[1,0].legend()


plt.show()





xx = np.linspace(0, 1, 100)
# (1,1) element of CH
yy_lo_11 = y['low'][:,0,0].reshape((-1, 1))
yy_lo_12 = y['low'][:,0,1].reshape((-1, 1))
yy_lo_33 = y['low'][:,2,2].reshape((-1, 1))

train_index_lo = [int(np.floor(i)) for i in np.linspace(0,99,20)]
x_train_lo = xx[train_index_lo].reshape((-1, 1))
y_train_lo = np.concatenate((yy_lo_11,yy_lo_12,yy_lo_33),axis=1)[train_index_lo]

x_test = xx.reshape((-1, 1))
y_test_lo = np.concatenate((yy_lo_11,yy_lo_12,yy_lo_33),axis=1)






# model_L: nn_l
model_L = NN1(1, 20, 4, 3)
model_L.apply(weights_init)
optimizer = optim.Adam(model_L.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()
x_lo_r = torch.from_numpy(x_train_lo).float()
x_lo_r.requires_grad_()
it = 0
loss = 1
nIter1 = 15000
while loss > 2e-5 and it < nIter1:
    pred_h = model_L(x_lo_r)
    grads = autograd.grad(outputs=pred_h, inputs=x_lo_r,
                            grad_outputs=torch.ones_like(pred_h),
                            create_graph=True, retain_graph=True, only_inputs=True)[0]
    loss = loss_fn(pred_h, torch.from_numpy(y_train_lo).float())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    loss_value = loss.item()
    if it % 50 == 0:
        print('It:', it, 'Loss', loss.item())
    it = it + 1



nn_pred_l = model_L(torch.from_numpy(x_test).float()).detach().numpy()


# plot
yy_hi_11 = y['hi'][:,0,0].reshape((-1, 1))
yy_hi_12 = y['hi'][:,0,1].reshape((-1, 1))
yy_hi_33 = y['hi'][:,2,2].reshape((-1, 1))
y_test_hi = np.concatenate((yy_hi_11,yy_hi_12,yy_hi_33),axis=1)

fig, axes = plt.subplots(2,2,figsize=(15, 15))

axes[0,0].plot(x_test, y_test_lo[:,0], color='blue', linestyle='--', linewidth=2.0,label='ref_lo')
axes[0,0].plot(x_test, y_test_hi[:,0], color='red', linestyle='--', linewidth=2.0,label='ref_hi')
axes[0,0].plot(x_test, nn_pred_l[:,0],color='green', linewidth=2.0,label='NN')
axes[0,0].scatter(x_train_lo, y_train_lo[:,0],color='black', linewidth=2.0,label='sample')
axes[0,0].legend()
axes[0,0].set_title("components of CH[0,0]")

axes[0,1].plot(x_test, y_test_lo[:,1],color='blue', linestyle='--', linewidth=2.0,label='ref_lo')
axes[0,1].plot(x_test, y_test_hi[:,1], color='red', linestyle='--', linewidth=2.0,label='ref_hi')
axes[0,1].plot(x_test, nn_pred_l[:,1],color='green', linewidth=2.0,label='NN')
axes[0,1].scatter(x_train_lo, y_train_lo[:,1],color='black', linewidth=2.0,label='sample')
axes[0,1].legend()
axes[0,1].set_title("components of CH[0,1]")

axes[1,0].plot(x_test, y_test_lo[:,2],color='blue', linestyle='--', linewidth=2.0,label='ref_lo')
axes[1,0].plot(x_test, y_test_hi[:,2], color='red', linestyle='--', linewidth=2.0,label='ref_hi')
axes[1,0].plot(x_test, nn_pred_l[:,2],color='green', linewidth=2.0,label='NN')
axes[1,0].scatter(x_train_lo, y_train_lo[:,2],color='black', linewidth=2.0,label='sample')
axes[1,0].legend()
axes[1,0].set_title("components of CH[2,2]")

plt.show()





xx = np.linspace(0., 1, 100)
# (1,1) element of CH
yy_lo_11 = y['low'][:,0,0].reshape((-1, 1))
yy_lo_12 = y['low'][:,0,1].reshape((-1, 1))
yy_lo_33 = y['low'][:,2,2].reshape((-1, 1))

train_index_lo = [int(np.floor(i)) for i in np.linspace(0,99,20)]
x_train_lo = xx[train_index_lo].reshape((-1, 1))
y_train_lo = np.concatenate((yy_lo_11,yy_lo_12,yy_lo_33),axis=1)[train_index_lo]


yy_hi_11 = y['hi'][:,0,0].reshape((-1, 1))
yy_hi_12 = y['hi'][:,0,1].reshape((-1, 1))
yy_hi_33 = y['hi'][:,2,2].reshape((-1, 1))

train_index_hi = [0,29,59,79,89,99]
# train_index_hi = [int(np.floor(i)) for i in np.linspace(0,99,6)]
x_train_hi = xx[train_index_hi].reshape((-1, 1))
y_train_hi = np.concatenate((yy_hi_11,yy_hi_12,yy_hi_33),axis=1)[train_index_hi]


x_test = xx.reshape((-1, 1))
y_test_lo = np.concatenate((yy_lo_11,yy_lo_12,yy_lo_33),axis=1)
y_test_hi = np.concatenate((yy_hi_11,yy_hi_12,yy_hi_33),axis=1)


train_index_hi


# dnn with linear sub nn
model_noLinear = NN1(4, 20, 4, 3)
model_noLinear.apply(weights_init)
model_Linear = NN2(4, 10, 2, 3)
model_Linear.apply(weights_init)
alpha = torch.tensor([0.5,0.5,0.5])
# model_Alpha = nn.Linear(6,3,bias=False)
optimizer2 = optim.Adam([{'params': model_noLinear.parameters()},
                         {'params': model_Linear.parameters()},
                         # {'params': model_L.parameters(), 'weight_decay': 0.01},
                         ], lr=1e-3)
nIter2 = 5000
x_lo_r = torch.from_numpy(x_train_lo).float()
x_lo_r.requires_grad_()
loss2_value = 1
it = 0
# x_lo_r.requires_grad_()
while loss2_value > 1e-9 and it < nIter2:
    # pred_h = model_L(x_lo_r)
    # grads = autograd.grad(outputs=pred_h, inputs=x_lo_r,
    #                         grad_outputs=torch.ones_like(pred_h),
    #                         create_graph=True, retain_graph=True, only_inputs=True)[0]
    # loss3 = torch.mean(torch.square(pred_h - torch.from_numpy(y_train_lo).float()))

    pred_2h = model_L(torch.from_numpy(x_train_hi).float())
    pred_2 = alpha * model_noLinear(torch.cat((torch.from_numpy(x_train_hi).float(), pred_2h), 1)) +\
              (1 - alpha) * model_Linear(torch.cat((torch.from_numpy(x_train_hi).float(), pred_2h), 1))
    # pred_nl = model_noLinear(torch.cat((torch.from_numpy(x_train_hi).float(), pred_2h), 1))
    # pred_l = model_Linear(torch.cat((torch.from_numpy(x_train_hi).float(), pred_2h), 1))
    
    # pred_2 = model_Alpha(torch.cat((pred_nl,pred_l),1))
    loss2 = torch.mean(torch.square(pred_2 - torch.from_numpy(y_train_hi))) 
    loss2_value = loss2.item()
    optimizer2.zero_grad()
    loss2.backward()
    optimizer2.step()
    if it % 100 == 0:
        print('It:', it, 'Loss:', loss2.item())
    it = it + 1


y_lo_nn = model_L(torch.from_numpy(x_test.reshape((-1,1))).float())

y_hi_mfnn = alpha * model_noLinear(torch.cat((torch.from_numpy(x_test.reshape((-1,1))).float(), y_lo_nn), 1)) +\
              (1 - alpha) * model_Linear(torch.cat((torch.from_numpy(x_test.reshape((-1,1))).float(), y_lo_nn), 1))
# y_hi_mfnn = model_Alpha(torch.cat((model_noLinear(torch.cat((torch.from_numpy(x_test.reshape((-1,1))).float(), y_lo_nn), 1)),
#   model_Linear(torch.cat((torch.from_numpy(x_test.reshape((-1,1))).float(), y_lo_nn), 1))),1))
    
y_hi_mfnn = y_hi_mfnn.detach().numpy()


# plot

fig, axes = plt.subplots(2,2,figsize=(15, 15))

axes[0,0].plot(x_test, y_test_lo[:,0], color='blue', linestyle='--', linewidth=2.0,label='ref_lo')
axes[0,0].plot(x_test, y_test_hi[:,0], color='red', linestyle='--', linewidth=2.0,label='ref_hi')
axes[0,0].plot(x_test, nn_pred_l[:,0],color='green', linewidth=2.0,label='NN_L')
axes[0,0].plot(x_test, y_hi_mfnn[:,0],color='blue', linewidth=2.0,label='NN_MF')
axes[0,0].scatter(x_train_lo, y_train_lo[:,0],color='black', linewidth=2.0,label='sample_lo')
axes[0,0].scatter(x_train_hi, y_train_hi[:,0],marker='x',color='black', linewidth=2.0,label='sample_hi')
axes[0,0].legend()
axes[0,0].set_title("components of CH[0,0]")

axes[0,1].plot(x_test, y_test_lo[:,1],color='blue', linestyle='--', linewidth=2.0,label='ref_lo')
axes[0,1].plot(x_test, y_test_hi[:,1], color='red', linestyle='--', linewidth=2.0,label='ref_hi')
axes[0,1].plot(x_test, nn_pred_l[:,1],color='green', linewidth=2.0,label='NN_L')
axes[0,1].plot(x_test, y_hi_mfnn[:,1],color='blue', linewidth=2.0,label='NN_MF')
axes[0,1].scatter(x_train_lo, y_train_lo[:,1],color='black', linewidth=2.0,label='sample_lo')
axes[0,1].scatter(x_train_hi, y_train_hi[:,1],marker='x',color='black', linewidth=2.0,label='sample_hi')
axes[0,1].legend()
axes[0,1].set_title("components of CH[0,1]")

axes[1,0].plot(x_test, y_test_lo[:,2],color='blue', linestyle='--', linewidth=2.0,label='ref_lo')
axes[1,0].plot(x_test, y_test_hi[:,2], color='red', linestyle='--', linewidth=2.0,label='ref_hi')
axes[1,0].plot(x_test, nn_pred_l[:,2],color='green', linewidth=2.0,label='NN_L')
axes[1,0].plot(x_test, y_hi_mfnn[:,2],color='blue', linewidth=2.0,label='NN_MF')
axes[1,0].scatter(x_train_lo, y_train_lo[:,2],color='black', linewidth=2.0,label='sample_lo')
axes[1,0].scatter(x_train_hi, y_train_hi[:,2],marker='x',color='black', linewidth=2.0,label='sample_hi')
axes[1,0].legend()
axes[1,0].set_title("components of CH[2,2]")

plt.show()


x_predict_test = np.array([0.1,0.05,0.02,0.01,0.005,0.001,0])
y_predict_lo =  model_L(torch.from_numpy(x_predict_test.reshape((-1,1))).float())


y_predict_hi = alpha * model_noLinear(torch.cat((torch.from_numpy(x_predict_test.reshape((-1,1))).float(), y_predict_lo), 1)) +\
              (1 - alpha) * model_Linear(torch.cat((torch.from_numpy(x_predict_test.reshape((-1,1))).float(), y_predict_lo), 1))

    
y_predict_hi = y_predict_hi.detach().numpy()


y_predict_hi


torch.save(model_L, 'crossed_low.pth')
torch.save(model_Linear, 'crossed_hi_l.pth')
torch.save(model_noLinear, 'crossed_hi_nl.pth')



